import argparse
import json
import os
import concurrent.futures
import difflib

from patchpilot.reproduce.task import make_swe_tasks
from patchpilot.util.utils import setup_logger, ensure_directory_exists
from patchpilot.util.model import make_model
from patchpilot.reproduce.task import parse_task_list_file


def check_existing_verify_ids(verify_path):
    instance_ids = set()

    for root, _, files in os.walk(verify_path):
        for file in files:
            if file.endswith('verify_outputs.json'):
                file_path = os.path.join(root, file)
                with open(file_path, 'r', encoding='utf-8') as f:
                    try:
                        data = json.load(f)
                    except Exception as e:
                        print(f"Error loading {file_path}: {e}")
                        return instance_ids
                if 'instance_id' in data:
                    instance_ids.add(data['instance_id'])
    return instance_ids


class LLMVF:

    check_poc_prompt = """
    You are a code reviewer for a GitHub project. Your task is to evaluate whether a patch successfully resolves an issue described in a GitHub issue.

    You will be given the issue description, the PoC (Proof of Concept) code that demonstrates the issue, the PoC execution output before and after the patch, and the criteria for determining whether the issue described in the issue exists.

    Your goal is to determine if the patch resolves the issue. Please respond with "Yes" or "No" and provide a brief explanation of your reasoning.
    You should not assume that there is other output that is not shown in the Poc Output. The Poc Output is the only output you should consider. You should not assume that a plot is generated by the Poc.

    - "Yes" means the patch successfully fixed the issue.
    - "No" means the patch did not successfully fix the issue, either because the issue still exists or because the patch introduced new issues.
    
    ### Raw Issue Description ###
    {issue_description}

    ### Poc code ###
    Here is the PoC code that demonstrates the issue:
    {poc_code}
    
    ### Oracle ###
    The criteria for determining whether the issue exists are provided below. Note that these may not be exhaustive or entirely accurate, so please refer to the issue description for further clarification:
    {oracle}

    {wrong_behavior}

    {expected_behavior}

    ### Poc Output before the patch ###
    {old_execution_output}
    
    ### Poc Output after the patch ###
    {new_execution_output}

    **Response Format:**


    Example 1:
    <judgement> Yes </judgement>
    <explanation> The patch successfully fixed the issue since ... </explanation>
    Example 2:
    <judgement> No </judgement>
    <explanation> The patch did not successfully fix the issue since ... </explanation>

    """

    def __init__(
            self,
            instance_id,
            problem_statement,
            model_name,
            backend,
            logger,
    ):
        self.instance_id = instance_id
        self.problem_statement = problem_statement
        self.max_tokens = 4096
        self.model_name = model_name
        self.backend = backend
        self.logger = logger


def extract_failed_tests(output_string):
    failed_tests = []
    for line in output_string.splitlines():
        if line.strip().endswith(' E') or '... ERROR' in line or "... FAIL" in line:
            failed_tests.append(line.strip())
        elif 'FAILED' in line and "failures=" not in line:
            failed_tests.append(line.strip())
        elif line.strip().startswith("test_") and (line.strip().endswith(' F') or line.strip().endswith(' f')):
            failed_tests.append(line.strip())
    return failed_tests


def filter_functionality_test_output(old_output, new_output):
    old_failed_tests = extract_failed_tests(old_output)
    new_failed_tests = extract_failed_tests(new_output)

    return old_failed_tests, new_failed_tests


def filter_poc_test_if_succeed(old_execution_output, new_execution_output):
    if 'error' in old_execution_output["stderr"].lower() or 'exception' in old_execution_output["stderr"].lower():
        # if the old execution output has error or exception, and the new execution output has no error or exception,
        # then the test is passed
        if 'error' not in new_execution_output["stderr"].lower() and 'exception' not in new_execution_output["stderr"].lower():
            return True
    if old_execution_output["stdout"] != "":
        # if there is no error or exception in the old execution output, if the old execution output has stdout,
        # and the new execution output has different stdout (without error), then the test is passed
        if 'error' not in new_execution_output["stderr"].lower() and 'exception' not in new_execution_output["stderr"].lower() and new_execution_output["stdout"] != old_execution_output["stdout"]:
            return True
    return False


def execute_poc_test(task, args, logger, reproduce_info):
    llm_judgement = False
    execution_output = task.execute_poc(reproduce_info)
    if len(execution_output["stdout"]) > 500:
        execution_output["stdout"] = execution_output["stdout"][-500:]
    if len(execution_output["stderr"]) > 500:
        execution_output["stderr"] = execution_output["stderr"][-500:]

    rp = LLMVF(
        task.task_id,
        reproduce_info["result"]["oracle"]["issue_description"],
        args.model,
        args.backend,
        logger,
    )
    print(reproduce_info["result"]["oracle"]["execution_output"])
    rule_based_judgement = filter_poc_test_if_succeed(reproduce_info["result"]["oracle"]["execution_output"],
                                                    execution_output)
    
    wrong_behavior = reproduce_info["result"]["oracle"].get("wrong_behavior", "")
    expected_behavior = reproduce_info["result"]["oracle"].get("expected_behavior", "")
    oracle_description = reproduce_info["result"]["oracle"].get("oracle_description", "")
    
    wrong_behavior_prompt = ""
    expected_behavior_prompt = ""
    oracle_prompt = ""
    
    if wrong_behavior:
        wrong_behavior_prompt = f"**Here is the wrong behavior reported by a user before the patch:**\n{wrong_behavior}\n\n" if wrong_behavior else ""
    if expected_behavior:
        expected_behavior_prompt = f"**Here is the expected behavior:**\n{expected_behavior}\n\n" if expected_behavior else ""
    if oracle_description:
        oracle_prompt = f"**Here are the criteria for determining whether the issue described in the issue description exists:**\n{oracle_description}\n\n" if oracle_description else ""
    
    if not wrong_behavior and not expected_behavior and not oracle_description:
        oracle_prompt = "The criteria for determining whether the issue exists are not provided. Please refer to the issue description."
    
    message = rp.check_poc_prompt.format(
        issue_description=task.problem_statement,
        poc_code=reproduce_info["result"]["poc"]["poc_code"],
        oracle=oracle_prompt,
        wrong_behavior=wrong_behavior_prompt,
        expected_behavior=expected_behavior_prompt,
        old_execution_output={
            "stdout": reproduce_info["result"]["oracle"]["execution_output"]["stdout"][-500:],
            "stderr": reproduce_info["result"]["oracle"]["execution_output"]["stderr"][-500:],
        },
        new_execution_output=execution_output,
    ).strip()
    rp.logger.info(f"prompting with message:\n{message}")
    print(f"prompting with message:\n{message}")
    rp.logger.info("=" * 80)
    print("=" * 80)

    model = make_model(
        model=rp.model_name,
        backend=rp.backend,
        logger=rp.logger,
        max_tokens=rp.max_tokens,
        temperature=0,
        batch_size=1,
    )
    traj = model.codegen(message, num_samples=1)[0]
    rp.logger.info(f"Got response:\n{traj}")
    traj["prompt"] = message
    raw_output = traj["response"]
    print(raw_output)
    judgement = ""
    reason = ""
    try:
        judgement = raw_output.split("<judgement>")[1].split("</judgement>")[0].strip()
        reason = raw_output.split("<explanation>")[1].split("</explanation>")[0].strip()
    except Exception as e:
        print(
            "Error in parsing the output of the LLM, no judgement and reason found. The raw output is: {raw_output}")
        logger.error(
            f"Error in parsing the output of the LLM, no judgement and reason found. The raw output is: {raw_output}")
    if "yes" in judgement.lower():
        llm_judgement = True
    print(f"judgement: {judgement}, reason: {reason}")
    return execution_output, rule_based_judgement, llm_judgement, reason

def execute_verify_instance(task, args, existing_instance_ids):

    verify_info = {
        "instance_id": task.task_id,
        "result": {
            "poc_test_succeed_llm": [],
            "llm_judgement_reason": [],
            "poc_test_succeed_rule": [],
            "poc_is_executed": [],
            "poc_code": [],
            "poc_execution_output": [],
            "functionality_test_command": "",
            "functionality_test_output_ex": {
                "stdout": "",
                "stderr": "",
            },
            "functionality_test_output": {
                "stdout": "",
                "stderr": "",
            },
            "functionality_test_fail_num": {
                "old_failed_tests_num": 0,
                "new_failed_tests_num": 100,
            },
        }
    }

    log_file = os.path.join(
        args.verify_folder, f"{task.task_id}.log"
    )
    logger = setup_logger(log_file)
    logger.info(f"Processing bug {task.task_id}")

    if task.task_id in existing_instance_ids:
        print(f"Skip verifying existing instance_id: {task.task_id}")
        logger.info(f"Skip verifying existing instance_id: {task.task_id}")
        return
    
    verify_issue_id_folder = os.path.join(args.verify_folder, task.task_id)
    ensure_directory_exists(verify_issue_id_folder)
    verify_output_file = os.path.join(verify_issue_id_folder, "verify_outputs.json")
    
    task.reset_project()
    func_test_output_ex = task.execute_functionality_test()
    verify_info["result"]["functionality_test_output_ex"] = func_test_output_ex
    task.apply_patch()
    func_test_output = task.execute_functionality_test()
    verify_info["result"]["functionality_test_output"] = func_test_output
    verify_info["result"]["functionality_test_command"] = task.test_cmd
    verify_info["result"]["poc_is_executed"] = []
    verify_info["result"]["poc_execution_output"] = []
    verify_info["result"]["poc_test_succeed_rule"] = []
    verify_info["result"]["poc_test_succeed_llm"] = []
    verify_info["result"]["llm_judgement_reason"] = []
    verify_info["result"]["poc_code"] = []
    
    reproduce_issue_id_folder = os.path.join(args.reproduce_folder, task.task_id)
    ensure_directory_exists(reproduce_issue_id_folder)
    
    poc_index = 0
    while os.path.exists(os.path.join(reproduce_issue_id_folder, f"issue_parsing_report_{poc_index}.json")): 
        reproduce_output_file = os.path.join(reproduce_issue_id_folder, f"issue_parsing_report_{poc_index}.json")
        with open(reproduce_output_file, "r") as f:
            reproduce_info = json.load(f)
        verify_info["result"]["poc_code"].append(reproduce_info["result"]["poc"]["poc_code"])
        if reproduce_info["result"]["oracle"]["exec_match_wrong_behavior"]:  # if the generated PoC code matches the issue description
            poc_code_dict = reproduce_info["result"]["poc"]["poc_code"]
            _, poc_code = next(iter(poc_code_dict.items()))
            task.dump_poc(poc_code)
            execution_output, rule_based_judgement, llm_judgement, reason = execute_poc_test(task, args, logger, reproduce_info)
            verify_info["result"]["poc_is_executed"].append(True)
            verify_info["result"]["poc_execution_output"].append(execution_output)
            verify_info["result"]["poc_test_succeed_rule"].append(rule_based_judgement)
            verify_info["result"]["poc_test_succeed_llm"].append(llm_judgement)
            verify_info["result"]["llm_judgement_reason"].append(reason)
            verify_info["result"]["poc_code"].append(poc_code)
        else:
            break
        poc_index += 1                

    old_test_output = verify_info["result"]["functionality_test_output_ex"]["stdout"] + verify_info["result"]["functionality_test_output_ex"]["stderr"]
    new_test_output = verify_info["result"]["functionality_test_output"]["stdout"] + verify_info["result"]["functionality_test_output"]["stderr"]
    old_failed_tests, new_failed_tests = filter_functionality_test_output(old_test_output, new_test_output)
    functionality_test_fail_num = {
        "old_failed_tests_num": len(old_failed_tests),
        "new_failed_tests_num": len(new_failed_tests)
    }
    diff = difflib.ndiff(old_failed_tests, new_failed_tests)
    diff_str = '\n'.join(line for line in diff if line.startswith(('+', '-')))
    verify_info["result"]["functionality_test_fail_num"] = functionality_test_fail_num
    verify_info["result"]["functionality_test_fail_diff_only_func"] = diff_str
    verify_info["result"]["old_failed_tests"] = '\n'.join(old_failed_tests)
    verify_info["result"]["new_failed_tests"] = '\n'.join(new_failed_tests)
    verify_info["patched_diff"] = task.patched_diff
    
    diff_whole = difflib.ndiff(old_test_output.splitlines(), new_test_output.splitlines())
    if len(old_test_output.splitlines()) > 1000 or len(new_test_output.splitlines()) > 1000:
        diff_whole = 'diff too long, skip'
    else:
        diff_whole = '\n'.join(line[1:] for line in diff_whole if line.startswith('+'))
    verify_info["result"]["functionality_test_fail_diff_whole"] = diff_whole

    with open(verify_output_file, "w") as ft:
        json.dump(verify_info, ft, indent=4)
    print("execute verifier for issue {} success! The result is in {}.".format(task.task_id, verify_output_file))


def verify(args):
    patch_list = []
    with open(args.patch_file, "r") as f:
        json_list = list(f)
    for json_str in json_list:
        result = json.loads(json_str)
        patch_list.append(result)

    for patch in patch_list:
        patch_instance_id = patch.get("instance_id")
        patch_diff = patch.get("model_patch")
        for task in args.tasks_list:
            if task.task_id == patch_instance_id:
                task.patched_diff = patch_diff
    existing_instance_ids = (
        check_existing_verify_ids(args.verify_folder)
    )

    if args.num_threads == 1:
        for task in args.tasks_list:
            execute_verify_instance(
                task, args, existing_instance_ids
            )
    else:
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=args.num_threads
        ) as executor:
            futures = [
                executor.submit(
                    execute_verify_instance,
                    task,
                    args,
                    existing_instance_ids
                )
                for task in args.tasks_list
            ]
            concurrent.futures.wait(futures)
            # for future in futures:
            #     result = future.result()  # this will raise an exception if the task generated an exception


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--verify_folder", type=str, required=True)
    parser.add_argument("--reproduce_folder", type=str, required=True)
    parser.add_argument("--patch_file", type=str, required=True)
    parser.add_argument(
        "--setup_map",
        type=str,
        help="Path to json file that contains the setup information of the projects.",
    )
    parser.add_argument(
        "--tasks_map",
        type=str,
        help="Path to json file that contains the tasks information.",
    )
    parser.add_argument(
        "--task_list_file",
        type=str,
        help="Path to the file that contains all tasks ids to be run.",
    )
    parser.add_argument("--target_id", type=str)
    parser.add_argument(
        "--num_threads",
        type=int,
        default=1,
        help="Number of threads to use for creating API requests",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-2024-08-06",
    )
    parser.add_argument(
        "--backend", type=str, default="openai", choices=["openai", "deepseek", "claude"]
    )

    args = parser.parse_args()

    assert (not "deepseek" in args.model) or (
            args.backend == "deepseek"
    ), "Must specify `--backend deepseek` if using a DeepSeek model"

    assert (not "claude" in args.model) or (
            args.backend == "claude"
    ), "Must specify `--backend claude` if using a Claude model"

    os.makedirs(args.verify_folder, exist_ok=True)

    # write the arguments
    with open(f"{args.verify_folder}/verify_args.json", "w") as f:
        json.dump(vars(args), f, indent=4)
    
    assert not (args.target_id is not None and args.task_list_file is not None), "Cannot specify both task and task-list."
    all_task_ids = []
    if args.task_list_file is not None:
        all_task_ids = parse_task_list_file(args.task_list_file)
    elif args.target_id is not None:
        all_task_ids = [args.target_id]
    assert len(all_task_ids) > 0, "No task ids to run."
    args.tasks_list = make_swe_tasks(all_task_ids, args.setup_map, args.tasks_map)

    verify(args)


if __name__ == "__main__":
    main()
